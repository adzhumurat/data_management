# Сервис агрегации пользовательских оценок

## Идея сервиса

Среди данных в наборе [The Movies Dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset) присутствует файл `ratings.csv`.
Этот файл хранит информацию об оценках пользователей в виде "плоской" структуры, где каждая строчка файла содержит событие вида: пользователь с идентификатором `uid_i` проставил контенту `cid_j` оценку `r_ij` в момент времени `t_ij`

| userId | movieId | rating | timestamp |
| --- | --- | --- | --- |
| uid_1 | mid_11 | r_11 | ts_11 |
| uid_1 | mid_12 | r_12 | ts_12 |
| uid_1 | mid_13 | r_13 | ts_13 |
| ... | ... | ... | ... |
| uid_n | mid_n1 | r_n1 | ts_n1 |
| uid_n | mid_n2 | r_n2 | ts_n2 |
| uid_n | mid_n3 | r_n3 | ts_n3 |
| uid_n | mid_n4 | r_n4 | ts_n4 |

Примерно в таком виде хранит пользовательский фидбэк [онлайн-кинотеатр ivi](https://www.ivi.ru/). Плюс такой структуры хранения данных, когда отдельному логическому событию соответствует одна запись в источнике данных, состоит в "атомарности" хранения - например, вы можете очень быстро вычистить записи по конкретным `uid` если узнаете, что эти оценки принадлежат ботам и являются результатом накрутки. Или, например, можно быстро "схлопнуть" задвоенные записи, которые являются в результатом сбоя.

В рамках pet-project предлагается создать сервис по агрегции пользовательских оценок, который будет представлять собой простое API, принимающее запросы вида
<pre>
curl -s "http://youservice/rates/uid"
</pre>

Для каждого переданного uid сервис будет выводить историю оценок, которые ставил этот пользователь, в виде JSON
<pre>
[
    {"movie_id": 4119470, "rating": 4, "timestamp": "2019-09-03 10:00:00"},
    {"movie_id": 5691170, "rating": 2, "timestamp": "2019-09-05 13:23:00"},
    {"movie_id": 3341191, "rating": 5, "timestamp": "2019-09-08 16:40:00"}
]
</pre>

## Детали реализации

Проект должен использовать технологию виртуалиции Docker и включать в себя несколько компонентов

* http-сервер на Python для ответа на внешние запросы
* Postgres для долгосрочного хранения данных из файла `ratings.csv`
* Redis для кеширования запросов в Postgres

Компоненты должны соединяться с помощью конфигурационного файла `docker-compose.yml`.

Пайплайн работы сервиса:

* с помощью docker-compose поднимаем Postgres. Создаём в базе таблицу ratings
* монтируем в контейнер с Postgres внешнюю директорию (пустую) `pg_data` для файлов БД Postgres
* монтируем в контейнер с http-сервером монтируем внешнюю директорию, которая содержит файл `ratings.csv`
* в контейнере с http-сервером на Python запускаем загрузку данных `\\copy ratings FROM '/data/ratings.csv' DELIMITER ',' CSV HEADER`
* с помощью docker-compose поднимаем контейнер с Redis - туда будем кешировать результаты запросов к Postgres
* запускаем http-сервер, порт из контейнера должен быть прокинут в хост-машину, на которой запускается докер 

Когда проект развернут, алгоритм работы следующий

* http-сервер принимает запрос, достаём из запроса user_id
* проверяем, есть ли в Redis результаты истории смотрения этого пользователя. Если есть - отвечаем на запрос
* если данных в кеше нет - инициируем запрос к Postgres, кладём результаты в Redis

Таким образом в результате этого проекта будет реализован сервис агрегации пользовательских оценок на базе Postgres с кеширующим сервером Redis.